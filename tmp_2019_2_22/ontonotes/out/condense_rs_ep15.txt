
train results at No. of hidden layers: 3, No. of training epochs: 15, No. of training size: 64
loss: 456.592, F1: 88.002, P: 88.361, R: 87.733
loss: 314.832, F1: 92.037, P: 91.999, R: 92.099
loss: 257.131, F1: 93.339, P: 93.224, R: 93.475
loss: 227.093, F1: 94.278, P: 94.311, R: 94.263
loss: 200.493, F1: 94.983, P: 95.194, R: 94.789
loss: 184.978, F1: 95.366, P: 95.482, R: 95.267
loss: 165.674, F1: 95.814, P: 95.830, R: 95.810
loss: 153.449, F1: 96.159, P: 96.276, R: 96.054
loss: 141.830, F1: 96.328, P: 96.510, R: 96.159
loss: 134.451, F1: 96.597, P: 96.739, R: 96.467
loss: 129.736, F1: 96.693, P: 96.833, R: 96.564
loss: 122.040, F1: 96.838, P: 96.929, R: 96.758
loss: 120.247, F1: 96.953, P: 97.007, R: 96.909
loss: 119.207, F1: 97.006, P: 97.099, R: 96.922
loss: 116.974, F1: 97.052, P: 97.169, R: 96.945

dev results at No. of hidden layers: 3, No. of training epochs: 15, No. of training size: 64
loss: 734.303, F1: 91.013, P: 89.796, R: 92.281
loss: 624.395, F1: 92.558, P: 91.502, R: 93.647
loss: 542.516, F1: 93.035, P: 92.018, R: 94.083
loss: 547.320, F1: 93.459, P: 92.828, R: 94.107
loss: 564.918, F1: 93.696, P: 92.974, R: 94.437
loss: 553.776, F1: 93.589, P: 92.768, R: 94.433
loss: 566.340, F1: 93.721, P: 92.944, R: 94.517
loss: 584.668, F1: 93.757, P: 93.073, R: 94.457
loss: 594.589, F1: 93.785, P: 93.251, R: 94.331
loss: 589.081, F1: 93.853, P: 93.249, R: 94.471
loss: 597.009, F1: 93.872, P: 93.271, R: 94.486
loss: 602.692, F1: 93.904, P: 93.296, R: 94.526
loss: 620.589, F1: 93.899, P: 93.249, R: 94.564
loss: 619.087, F1: 93.898, P: 93.289, R: 94.520
loss: 626.198, F1: 93.929, P: 93.337, R: 94.535

test results at No. of hidden layers: 3, No. of training epochs: 15, No. of training size: 64
loss: 663.912, F1: 92.123, P: 90.998, R: 93.285
loss: 624.395, F1: 92.558, P: 91.502, R: 93.647
loss: 542.516, F1: 93.035, P: 92.018, R: 94.083
loss: 547.320, F1: 93.459, P: 92.828, R: 94.107
loss: 564.918, F1: 93.696, P: 92.974, R: 94.437
loss: 553.776, F1: 93.589, P: 92.768, R: 94.433
loss: 566.340, F1: 93.721, P: 92.944, R: 94.517
loss: 584.668, F1: 93.757, P: 93.073, R: 94.457
loss: 594.589, F1: 93.785, P: 93.251, R: 94.331
loss: 589.081, F1: 93.853, P: 93.249, R: 94.471
loss: 597.009, F1: 93.872, P: 93.271, R: 94.486
loss: 602.692, F1: 93.904, P: 93.296, R: 94.526
loss: 620.589, F1: 93.899, P: 93.249, R: 94.564
loss: 619.087, F1: 93.898, P: 93.289, R: 94.520
loss: 626.198, F1: 93.929, P: 93.337, R: 94.535

train results at No. of hidden layers: 6, No. of training epochs: 15, No. of training size: 64
loss: 349.435, F1: 90.797, P: 90.317, R: 91.341
loss: 225.912, F1: 93.778, P: 93.185, R: 94.417
loss: 175.708, F1: 95.255, P: 95.262, R: 95.268
loss: 142.657, F1: 95.964, P: 95.713, R: 96.229
loss: 116.108, F1: 96.721, P: 96.768, R: 96.685
loss: 104.053, F1: 96.986, P: 96.875, R: 97.106
loss: 79.397, F1: 97.767, P: 97.919, R: 97.623
loss: 76.411, F1: 97.853, P: 97.858, R: 97.855
loss: 63.584, F1: 98.179, P: 98.209, R: 98.155
loss: 61.914, F1: 98.237, P: 98.197, R: 98.282
loss: 51.314, F1: 98.537, P: 98.531, R: 98.547
loss: 50.431, F1: 98.510, P: 98.521, R: 98.503
loss: 49.786, F1: 98.511, P: 98.436, R: 98.590
loss: 43.443, F1: 98.718, P: 98.730, R: 98.710
loss: 44.037, F1: 98.670, P: 98.646, R: 98.699

dev results at No. of hidden layers: 6, No. of training epochs: 15, No. of training size: 64
loss: 550.846, F1: 93.150, P: 92.140, R: 94.202
loss: 472.622, F1: 94.351, P: 93.301, R: 95.444
loss: 482.575, F1: 94.557, P: 93.729, R: 95.409
loss: 487.398, F1: 94.810, P: 93.934, R: 95.711
loss: 503.238, F1: 95.103, P: 94.514, R: 95.706
loss: 513.740, F1: 95.139, P: 94.490, R: 95.804
loss: 556.006, F1: 95.358, P: 94.770, R: 95.959
loss: 578.492, F1: 95.274, P: 94.585, R: 95.980
loss: 574.916, F1: 95.492, P: 94.851, R: 96.149
loss: 612.129, F1: 95.489, P: 94.805, R: 96.189
loss: 631.551, F1: 95.572, P: 95.010, R: 96.146
loss: 647.736, F1: 95.554, P: 94.922, R: 96.201
loss: 677.469, F1: 95.537, P: 94.888, R: 96.200
loss: 692.436, F1: 95.588, P: 94.936, R: 96.255
loss: 692.113, F1: 95.589, P: 94.940, R: 96.253

test results at No. of hidden layers: 6, No. of training epochs: 15, No. of training size: 64
loss: 489.425, F1: 94.074, P: 93.132, R: 95.047
loss: 472.622, F1: 94.351, P: 93.301, R: 95.444
loss: 482.575, F1: 94.557, P: 93.729, R: 95.409
loss: 487.398, F1: 94.810, P: 93.934, R: 95.711
loss: 503.238, F1: 95.103, P: 94.514, R: 95.706
loss: 513.740, F1: 95.139, P: 94.490, R: 95.804
loss: 556.006, F1: 95.358, P: 94.770, R: 95.959
loss: 578.492, F1: 95.274, P: 94.585, R: 95.980
loss: 574.916, F1: 95.492, P: 94.851, R: 96.149
loss: 612.129, F1: 95.489, P: 94.805, R: 96.189
loss: 631.551, F1: 95.572, P: 95.010, R: 96.146
loss: 647.736, F1: 95.554, P: 94.922, R: 96.201
loss: 677.469, F1: 95.537, P: 94.888, R: 96.200
loss: 692.436, F1: 95.588, P: 94.936, R: 96.255
loss: 692.113, F1: 95.589, P: 94.940, R: 96.253
